Day026
================

HW (Kaggle)計程車費率預測
-------------------------

<https://www.kaggle.com/c/new-york-city-taxi-fare-prediction>

Packages loading

``` r
library(lubridate)
library(tidyverse)
library(caret)
```

Data loading

``` r
df <- read.csv("data/taxi_data1.csv")
train_Y <- df %>% select("fare_amount")
df <- df %>% select(-"fare_amount")
sapply(list(df=df), dim) %>% 'rownames<-'(c('nrow','ncol')) 
```

    ##        df
    ## nrow 5000
    ## ncol    6

``` r
df %>% head
```

    ##           pickup_datetime pickup_longitude pickup_latitude
    ## 1 2011-10-21 23:54:10 UTC        -73.99058        40.76107
    ## 2 2015-02-03 10:42:03 UTC        -73.98840        40.72343
    ## 3 2014-03-16 18:58:58 UTC        -74.01578        40.71511
    ## 4 2009-06-13 16:10:54 UTC        -73.97732        40.78728
    ## 5 2014-06-12 03:25:56 UTC        -73.98968        40.72972
    ## 6 2011-07-16 01:19:59 UTC        -73.99763        40.72181
    ##   dropoff_longitude dropoff_latitude passenger_count
    ## 1         -73.98113         40.75863               2
    ## 2         -73.98965         40.74170               1
    ## 3         -74.01203         40.70789               2
    ## 4         -73.95803         40.77884               3
    ## 5         -73.98249         40.76189               3
    ## 6         -74.00976         40.70994               1

時間特徵分解方式:使用package: lubridate

``` r
df$pickup_datetime <- as.POSIXct(df$pickup_datetime, format='%Y-%m-%d %H:%M:%S UTC')
df$pickup_year <- year(df$pickup_datetime) 
df$pickup_month <- month(df$pickup_datetime) 
df$pickup_day <- day(df$pickup_datetime) 
df$pickup_hour <- hour(df$pickup_datetime) 
df$pickup_minute <- minute(df$pickup_datetime) 
df$pickup_second <- second(df$pickup_datetime) 
df %>% head
```

    ##       pickup_datetime pickup_longitude pickup_latitude dropoff_longitude
    ## 1 2011-10-21 23:54:10        -73.99058        40.76107         -73.98113
    ## 2 2015-02-03 10:42:03        -73.98840        40.72343         -73.98965
    ## 3 2014-03-16 18:58:58        -74.01578        40.71511         -74.01203
    ## 4 2009-06-13 16:10:54        -73.97732        40.78728         -73.95803
    ## 5 2014-06-12 03:25:56        -73.98968        40.72972         -73.98249
    ## 6 2011-07-16 01:19:59        -73.99763        40.72181         -74.00976
    ##   dropoff_latitude passenger_count pickup_year pickup_month pickup_day
    ## 1         40.75863               2        2011           10         21
    ## 2         40.74170               1        2015            2          3
    ## 3         40.70789               2        2014            3         16
    ## 4         40.77884               3        2009            6         13
    ## 5         40.76189               3        2014            6         12
    ## 6         40.70994               1        2011            7         16
    ##   pickup_hour pickup_minute pickup_second
    ## 1          23            54            10
    ## 2          10            42             3
    ## 3          18            58            58
    ## 4          16            10            54
    ## 5           3            25            56
    ## 6           1            19            59

``` r
df_temp <- df %>% select(-"pickup_datetime")

# 最小最大化
normal <- preProcess(df_temp, method = "range", rangeBounds = c(0,1))
train_X <- predict(normal, df_temp)
```

線性迴歸

``` r
train <- data.frame(train_X, fare_amount = train_Y)

control <- trainControl(method="cv", number=5, repeats = 5)
model.lr <- train(fare_amount~., data = train, method = 'glm', trControl=control)
model.lr
```

    ## Generalized Linear Model 
    ## 
    ## 5000 samples
    ##   11 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 4000, 4001, 4000, 4000, 3999 
    ## Resampling results:
    ## 
    ##   RMSE      Rsquared    MAE     
    ##   9.693225  0.03036774  6.019389

梯度提升樹

``` r
control <- trainControl(method="cv", number=5, repeats = 5)
model.gbm <- train(fare_amount~., data = train, method = "gbm", trControl = control, verbose = FALSE)
model.gbm
```

    ## Stochastic Gradient Boosting 
    ## 
    ## 5000 samples
    ##   11 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 4000, 4000, 4000, 4000, 4000 
    ## Resampling results across tuning parameters:
    ## 
    ##   interaction.depth  n.trees  RMSE      Rsquared   MAE     
    ##   1                   50      7.880759  0.3864258  5.078992
    ##   1                  100      7.425490  0.4333657  4.746781
    ##   1                  150      7.245973  0.4518695  4.606651
    ##   2                   50      6.376740  0.5966755  4.233938
    ##   2                  100      5.846899  0.6461800  3.823843
    ##   2                  150      5.688608  0.6607387  3.672978
    ##   3                   50      5.796477  0.6583688  3.807137
    ##   3                  100      5.516773  0.6810618  3.505048
    ##   3                  150      5.345236  0.6990484  3.339367
    ## 
    ## Tuning parameter 'shrinkage' was held constant at a value of 0.1
    ## 
    ## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were n.trees = 150,
    ##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.

增加緯度差, 經度差, 座標距離等三個特徵

``` r
df$longitude_diff <- df$dropoff_longitude - df$pickup_longitude
df$latitude_diff <- df$dropoff_latitude - df$pickup_latitude
df$distance_2D <- sqrt(df$longitude_diff^2 + df$latitude_diff^2)
df %>% 
  select(c("distance_2D","longitude_diff","latitude_diff","dropoff_longitude","pickup_longitude","dropoff_latitude","pickup_latitude")) %>%
  head
```

    ##   distance_2D longitude_diff latitude_diff dropoff_longitude
    ## 1 0.009761110    0.009452000   -0.00243700         -73.98113
    ## 2 0.018307058   -0.001243591    0.01826477         -73.98965
    ## 3 0.008140321    0.003756000   -0.00722200         -74.01203
    ## 4 0.021056216    0.019292000   -0.00843700         -73.95803
    ## 5 0.032964347    0.007193000    0.03217000         -73.98249
    ## 6 0.016964503   -0.012126000   -0.01186400         -74.00976
    ##   pickup_longitude dropoff_latitude pickup_latitude
    ## 1        -73.99058         40.75863        40.76107
    ## 2        -73.98840         40.74170        40.72343
    ## 3        -74.01578         40.70789        40.71511
    ## 4        -73.97732         40.77884        40.78728
    ## 5        -73.98968         40.76189        40.72972
    ## 6        -73.99763         40.70994        40.72181

``` r
df_temp <- df %>% select(-"pickup_datetime")

# 最小最大化
normal <- preProcess(df_temp, method = "range", rangeBounds = c(0,1))
train_X <- predict(normal, df_temp)
```

線性迴歸

``` r
train <- data.frame(train_X, fare_amount = train_Y)

control <- trainControl(method="cv", number=5, repeats = 5)
model.lr <- train(fare_amount~., data = train, method = 'glm', trControl=control)
model.lr
```

    ## Generalized Linear Model 
    ## 
    ## 5000 samples
    ##   14 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 3999, 4000, 4000, 4001, 4000 
    ## Resampling results:
    ## 
    ##   RMSE      Rsquared    MAE     
    ##   220.7505  0.03419462  17.28486

梯度提升樹

``` r
control <- trainControl(method="cv", number=5, repeats = 5)
model.gbm <- train(fare_amount~., data = train, method = "gbm", trControl = control, verbose = FALSE)
model.gbm
```

    ## Stochastic Gradient Boosting 
    ## 
    ## 5000 samples
    ##   14 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 4001, 3999, 4000, 4000, 4000 
    ## Resampling results across tuning parameters:
    ## 
    ##   interaction.depth  n.trees  RMSE      Rsquared   MAE     
    ##   1                   50      4.734792  0.7683137  2.585699
    ##   1                  100      4.549035  0.7803268  2.368356
    ##   1                  150      4.539429  0.7807465  2.340488
    ##   2                   50      4.503625  0.7848228  2.362902
    ##   2                  100      4.481153  0.7852938  2.276404
    ##   2                  150      4.489393  0.7842461  2.258571
    ##   3                   50      4.407698  0.7921492  2.298238
    ##   3                  100      4.428218  0.7899358  2.235156
    ##   3                  150      4.456891  0.7865744  2.231452
    ## 
    ## Tuning parameter 'shrinkage' was held constant at a value of 0.1
    ## 
    ## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were n.trees = 50, interaction.depth
    ##  = 3, shrinkage = 0.1 and n.minobsinnode = 10.

``` r
# 結果 : 準確度上升
```

作業1
-----

參考今日教材，試著使用經緯度一圈的長度比這一概念，組合出一個新特徵，再觀察原特徵加上新特徵是否提升了正確率?

Note: 地球的子午線總長度大約20003.93km
緯度1度 = 20003.93/180 = 大約111km
經度1度 = 110.574 = 大約111km
假設地球在赤道的半徑是R, 則赤道長度是2πR，緯度x度在地球上的圓周長是2πR\*cos(x) 有了圓周長2πRcos(x)，再切360度，則緯度x度時，經度x度的長度是2πRcos(x)/360

``` r
# 在緯度40.75度的經度與緯度比
lon_lat_ratio <- cospi(40.75/180)
df$distance_2D_rev <- sqrt((df$longitude_diff*lon_lat_ratio)^2 + df$latitude_diff^2)
df %>% head
```

    ##       pickup_datetime pickup_longitude pickup_latitude dropoff_longitude
    ## 1 2011-10-21 23:54:10        -73.99058        40.76107         -73.98113
    ## 2 2015-02-03 10:42:03        -73.98840        40.72343         -73.98965
    ## 3 2014-03-16 18:58:58        -74.01578        40.71511         -74.01203
    ## 4 2009-06-13 16:10:54        -73.97732        40.78728         -73.95803
    ## 5 2014-06-12 03:25:56        -73.98968        40.72972         -73.98249
    ## 6 2011-07-16 01:19:59        -73.99763        40.72181         -74.00976
    ##   dropoff_latitude passenger_count pickup_year pickup_month pickup_day
    ## 1         40.75863               2        2011           10         21
    ## 2         40.74170               1        2015            2          3
    ## 3         40.70789               2        2014            3         16
    ## 4         40.77884               3        2009            6         13
    ## 5         40.76189               3        2014            6         12
    ## 6         40.70994               1        2011            7         16
    ##   pickup_hour pickup_minute pickup_second longitude_diff latitude_diff
    ## 1          23            54            10    0.009452000   -0.00243700
    ## 2          10            42             3   -0.001243591    0.01826477
    ## 3          18            58            58    0.003756000   -0.00722200
    ## 4          16            10            54    0.019292000   -0.00843700
    ## 5           3            25            56    0.007193000    0.03217000
    ## 6           1            19            59   -0.012126000   -0.01186400
    ##   distance_2D distance_2D_rev
    ## 1 0.009761110     0.007563848
    ## 2 0.018307058     0.018289051
    ## 3 0.008140321     0.007762323
    ## 4 0.021056216     0.016875413
    ## 5 0.032964347     0.032628244
    ## 6 0.016964503     0.015004712

``` r
df_temp <- df %>% select(-"pickup_datetime")

# 最小最大化
normal <- preProcess(df_temp, method = "range", rangeBounds = c(0,1))
train_X <- predict(normal, df_temp)
```

線性迴歸

``` r
train <- data.frame(train_X, fare_amount = train_Y)

control <- trainControl(method="cv", number=5, repeats = 5)
model.lr <- train(fare_amount~., data = train, method = 'glm', trControl=control)
model.lr
```

    ## Generalized Linear Model 
    ## 
    ## 5000 samples
    ##   15 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 4000, 4000, 4001, 4000, 3999 
    ## Resampling results:
    ## 
    ##   RMSE      Rsquared    MAE     
    ##   9.646527  0.02736157  5.893555

梯度提升樹

``` r
control <- trainControl(method="cv", number=5, repeats = 5)
model.gbm <- train(fare_amount~., data = train, method = "gbm", trControl = control, verbose = FALSE)
model.gbm
```

    ## Stochastic Gradient Boosting 
    ## 
    ## 5000 samples
    ##   15 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 4001, 4000, 3999, 4000, 4000 
    ## Resampling results across tuning parameters:
    ## 
    ##   interaction.depth  n.trees  RMSE      Rsquared   MAE     
    ##   1                   50      4.748054  0.7689148  2.584560
    ##   1                  100      4.564720  0.7810978  2.364349
    ##   1                  150      4.541798  0.7828313  2.326131
    ##   2                   50      4.486982  0.7897167  2.360729
    ##   2                  100      4.417032  0.7945031  2.244939
    ##   2                  150      4.431130  0.7931335  2.223500
    ##   3                   50      4.398975  0.7962162  2.283474
    ##   3                  100      4.409458  0.7951333  2.208929
    ##   3                  150      4.436173  0.7933343  2.198545
    ## 
    ## Tuning parameter 'shrinkage' was held constant at a value of 0.1
    ## 
    ## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were n.trees = 50, interaction.depth
    ##  = 3, shrinkage = 0.1 and n.minobsinnode = 10.

``` r
# 結果 : 準確度上升
```

作業2
-----

試著只使用新特徵估計目標值(忽略原特徵)，效果跟作業1的結果比較起來效果如何?

``` r
df_temp <- df %>% select(-c("pickup_datetime","pickup_longitude","pickup_latitude","dropoff_longitude","dropoff_latitude","passenger_count"))
df_temp %>% head
```

    ##   pickup_year pickup_month pickup_day pickup_hour pickup_minute
    ## 1        2011           10         21          23            54
    ## 2        2015            2          3          10            42
    ## 3        2014            3         16          18            58
    ## 4        2009            6         13          16            10
    ## 5        2014            6         12           3            25
    ## 6        2011            7         16           1            19
    ##   pickup_second longitude_diff latitude_diff distance_2D distance_2D_rev
    ## 1            10    0.009452000   -0.00243700 0.009761110     0.007563848
    ## 2             3   -0.001243591    0.01826477 0.018307058     0.018289051
    ## 3            58    0.003756000   -0.00722200 0.008140321     0.007762323
    ## 4            54    0.019292000   -0.00843700 0.021056216     0.016875413
    ## 5            56    0.007193000    0.03217000 0.032964347     0.032628244
    ## 6            59   -0.012126000   -0.01186400 0.016964503     0.015004712

``` r
# 最小最大化
normal <- preProcess(df_temp, method = "range", rangeBounds = c(0,1))
train_X <- predict(normal, df_temp)
```

線性迴歸

``` r
train <- data.frame(train_X, fare_amount = train_Y)

control <- trainControl(method="cv", number=5, repeats = 5)
model.lr <- train(fare_amount~., data = train, method = 'glm', trControl=control)
model.lr
```

    ## Generalized Linear Model 
    ## 
    ## 5000 samples
    ##   10 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 4000, 4000, 4000, 4000, 4000 
    ## Resampling results:
    ## 
    ##   RMSE      Rsquared    MAE     
    ##   9.750019  0.03317997  5.904324

梯度提升樹

``` r
control <- trainControl(method="cv", number=5, repeats = 5)
model.gbm <- train(fare_amount~., data = train, method = "gbm", trControl = control, verbose = FALSE)
model.gbm
```

    ## Stochastic Gradient Boosting 
    ## 
    ## 5000 samples
    ##   10 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 4000, 4000, 4001, 3999, 4000 
    ## Resampling results across tuning parameters:
    ## 
    ##   interaction.depth  n.trees  RMSE      Rsquared   MAE     
    ##   1                   50      4.808336  0.7588818  2.553238
    ##   1                  100      4.676121  0.7680734  2.370826
    ##   1                  150      4.678812  0.7679259  2.345180
    ##   2                   50      4.572632  0.7787597  2.349062
    ##   2                  100      4.536496  0.7812105  2.268321
    ##   2                  150      4.536150  0.7814506  2.246448
    ##   3                   50      4.536094  0.7818262  2.290677
    ##   3                  100      4.553848  0.7798562  2.237857
    ##   3                  150      4.553628  0.7799375  2.223072
    ## 
    ## Tuning parameter 'shrinkage' was held constant at a value of 0.1
    ## 
    ## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were n.trees = 50, interaction.depth
    ##  = 3, shrinkage = 0.1 and n.minobsinnode = 10.

``` r
# 結果 : 準確度下降
```
