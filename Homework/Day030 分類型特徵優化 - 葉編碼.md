Day030
================

HW (Kaggle)鐵達尼生存預測精簡版
-------------------------------

<https://www.kaggle.com/c/titanic>

Packages loading

``` r
library(plyr)
library(tidyverse)
library(caret)
library(gbm)
library(mlbench)
library(pROC)
library(xgboost)
library(randomForest)
```

Data loading

``` r
df_train <- read.csv("data/titanic_train.csv")
sapply(list(df_train=df_train), dim) %>% 'rownames<-'(c('nrow','ncol')) 
```

    ##      df_train
    ## nrow      891
    ## ncol       12

Setting training data

``` r
train_y <- df_train$Survived
df <- df_train %>% select(-c("Survived","PassengerId"))
df %>% head
```

    ##   Pclass                                                Name    Sex Age
    ## 1      3                             Braund, Mr. Owen Harris   male  22
    ## 2      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38
    ## 3      3                              Heikkinen, Miss. Laina female  26
    ## 4      1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35
    ## 5      3                            Allen, Mr. William Henry   male  35
    ## 6      3                                    Moran, Mr. James   male  NA
    ##   SibSp Parch           Ticket    Fare Cabin Embarked
    ## 1     1     0        A/5 21171  7.2500              S
    ## 2     1     0         PC 17599 71.2833   C85        C
    ## 3     0     0 STON/O2. 3101282  7.9250              S
    ## 4     1     0           113803 53.1000  C123        S
    ## 5     0     0           373450  8.0500              S
    ## 6     0     0           330877  8.4583              Q

秀出資料欄位的類型與數量

``` r
table(sapply(df, class))
```

    ## 
    ##  factor integer numeric 
    ##       5       3       2

確定只有 integer, numeric, factor 三種類型後, 分別將欄位名稱存於三個 vector 中

``` r
feature_type <- sapply(df, class)
int_var <- feature_type[which(feature_type == "integer")] %>% as.data.frame %>% rownames
num_var <- feature_type[which(feature_type == "numeric")] %>% as.data.frame %>% rownames
fac_var <- feature_type[which(feature_type == "factor")] %>% as.data.frame %>% rownames
list(integer_feature = int_var,
     numeric_feature = num_var,
     factor_feature = fac_var)
```

    ## $integer_feature
    ## [1] "Pclass" "SibSp"  "Parch" 
    ## 
    ## $numeric_feature
    ## [1] "Age"  "Fare"
    ## 
    ## $factor_feature
    ## [1] "Name"     "Sex"      "Ticket"   "Cabin"    "Embarked"

把類別型特徵做標籤編碼

``` r
feature.names <- colnames(df[fac_var])
count <- 0

# Iterate through the columns
for (f in feature.names) {
  levels <- df[[f]] %>% unlist() %>% levels()
  df[[f]] <- mapvalues(df[[f]], from=levels, to=seq_along(levels)) %>% as.integer()
  count <- count + 1
}
df %>% head
```

    ##   Pclass Name Sex Age SibSp Parch Ticket    Fare Cabin Embarked
    ## 1      3  109   2  22     1     0    524  7.2500     1        4
    ## 2      1  191   1  38     1     0    597 71.2833    83        2
    ## 3      3  358   1  26     0     0    670  7.9250     1        4
    ## 4      1  277   1  35     1     0     50 53.1000    57        4
    ## 5      3   16   2  35     0     0    473  8.0500     1        4
    ## 6      3  559   2  NA     0     0    276  8.4583     1        3

``` r
# 缺失值補-1
df <- df %>% replace(., is.na(.), -1)

# 最小最大化
normal <- preProcess(df, method = "range", rangeBounds = c(0,1))
train_X <- predict(normal, df)

train <- train_X %>% mutate(Survived = as.factor(train_y))
levels(train$Survived) <- make.names(levels(factor(train$Survived)))
train %>% head
```

    ##   Pclass       Name Sex       Age SibSp Parch     Ticket       Fare
    ## 1      1 0.12134831   1 0.2839506 0.125     0 0.76911765 0.01415106
    ## 2      0 0.21348315   0 0.4814815 0.125     0 0.87647059 0.13913574
    ## 3      1 0.40112360   0 0.3333333 0.000     0 0.98382353 0.01546857
    ## 4      0 0.31011236   0 0.4444444 0.125     0 0.07205882 0.10364430
    ## 5      1 0.01685393   1 0.4444444 0.000     0 0.69411765 0.01571255
    ## 6      1 0.62696629   1 0.0000000 0.000     0 0.40441176 0.01650950
    ##       Cabin  Embarked Survived
    ## 1 0.0000000 1.0000000       X0
    ## 2 0.5578231 0.3333333       X1
    ## 3 0.0000000 1.0000000       X1
    ## 4 0.3809524 1.0000000       X1
    ## 5 0.0000000 1.0000000       X0
    ## 6 0.0000000 0.6666667       X0

Data Partition

``` r
inTrain <- createDataPartition(y=train$Survived, p=0.8, list=FALSE)
training <- train[inTrain,]; testing <- train[-inTrain,]
```

LR

``` r
control <- trainControl(method="cv", number=5,classProbs = TRUE, summaryFunction=twoClassSummary)
lr_model <- train(Survived~., data=training, method="glmnet", metric="ROC", trControl=control)
lr_model
```

    ## glmnet 
    ## 
    ## 714 samples
    ##  10 predictor
    ##   2 classes: 'X0', 'X1' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 571, 572, 571, 571, 571 
    ## Resampling results across tuning parameters:
    ## 
    ##   alpha  lambda        ROC        Sens       Spec     
    ##   0.10   0.0005316064  0.8471411  0.8500000  0.6929966
    ##   0.10   0.0053160643  0.8475168  0.8477273  0.7004040
    ##   0.10   0.0531606429  0.8467463  0.8681818  0.6858586
    ##   0.55   0.0005316064  0.8471396  0.8477273  0.6929966
    ##   0.55   0.0053160643  0.8465649  0.8454545  0.6931313
    ##   0.55   0.0531606429  0.8385032  0.8568182  0.6750842
    ##   1.00   0.0005316064  0.8471817  0.8454545  0.6893603
    ##   1.00   0.0053160643  0.8457430  0.8454545  0.7004714
    ##   1.00   0.0531606429  0.8361061  0.8568182  0.6787879
    ## 
    ## ROC was used to select the optimal model using the largest value.
    ## The final values used for the model were alpha = 0.1 and lambda
    ##  = 0.005316064.

``` r
lr_pred_prob <- predict(lr_model, testing, type = 'prob')
lr.ROC <- roc(response = testing$Survived,
               predictor = lr_pred_prob$X1,
               levels = levels(testing$Survived))
plot(lr.ROC, type="S", col="red"); text(x=0, y=.25, labels=paste("AUC =", round(lr.ROC$auc, 4)))
```

![](Day030_files/figure-markdown_github/unnamed-chunk-10-1.png)

GBDT

``` r
control <- trainControl(method="repeatedcv", number=5, repeats=5, classProbs = TRUE, summaryFunction=twoClassSummary)
xgb_model <- train(Survived ~ ., data = training, method = 'xgbTree', trControl = control, verbose = F, metric = 'ROC', nthread = 4)
xgb_model
```

    ## eXtreme Gradient Boosting 
    ## 
    ## 714 samples
    ##  10 predictor
    ##   2 classes: 'X0', 'X1' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 5 times) 
    ## Summary of sample sizes: 571, 571, 572, 571, 571, 571, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   eta  max_depth  colsample_bytree  subsample  nrounds  ROC      
    ##   0.3  1          0.6               0.50        50      0.8439619
    ##   0.3  1          0.6               0.50       100      0.8458081
    ##   0.3  1          0.6               0.50       150      0.8467743
    ##   0.3  1          0.6               0.75        50      0.8487191
    ##   0.3  1          0.6               0.75       100      0.8505032
    ##   0.3  1          0.6               0.75       150      0.8492125
    ##   0.3  1          0.6               1.00        50      0.8471882
    ##   0.3  1          0.6               1.00       100      0.8525230
    ##   0.3  1          0.6               1.00       150      0.8535955
    ##   0.3  1          0.8               0.50        50      0.8421579
    ##   0.3  1          0.8               0.50       100      0.8436765
    ##   0.3  1          0.8               0.50       150      0.8450927
    ##   0.3  1          0.8               0.75        50      0.8466009
    ##   0.3  1          0.8               0.75       100      0.8490510
    ##   0.3  1          0.8               0.75       150      0.8495372
    ##   0.3  1          0.8               1.00        50      0.8472729
    ##   0.3  1          0.8               1.00       100      0.8520535
    ##   0.3  1          0.8               1.00       150      0.8529128
    ##   0.3  2          0.6               0.50        50      0.8547332
    ##   0.3  2          0.6               0.50       100      0.8566953
    ##   0.3  2          0.6               0.50       150      0.8511208
    ##   0.3  2          0.6               0.75        50      0.8582003
    ##   0.3  2          0.6               0.75       100      0.8609727
    ##   0.3  2          0.6               0.75       150      0.8599034
    ##   0.3  2          0.6               1.00        50      0.8643942
    ##   0.3  2          0.6               1.00       100      0.8658633
    ##   0.3  2          0.6               1.00       150      0.8653232
    ##   0.3  2          0.8               0.50        50      0.8576234
    ##   0.3  2          0.8               0.50       100      0.8560782
    ##   0.3  2          0.8               0.50       150      0.8547207
    ##   0.3  2          0.8               0.75        50      0.8613949
    ##   0.3  2          0.8               0.75       100      0.8629454
    ##   0.3  2          0.8               0.75       150      0.8616760
    ##   0.3  2          0.8               1.00        50      0.8644826
    ##   0.3  2          0.8               1.00       100      0.8666983
    ##   0.3  2          0.8               1.00       150      0.8657576
    ##   0.3  3          0.6               0.50        50      0.8604250
    ##   0.3  3          0.6               0.50       100      0.8588886
    ##   0.3  3          0.6               0.50       150      0.8581864
    ##   0.3  3          0.6               0.75        50      0.8598656
    ##   0.3  3          0.6               0.75       100      0.8595866
    ##   0.3  3          0.6               0.75       150      0.8591307
    ##   0.3  3          0.6               1.00        50      0.8669131
    ##   0.3  3          0.6               1.00       100      0.8652196
    ##   0.3  3          0.6               1.00       150      0.8656838
    ##   0.3  3          0.8               0.50        50      0.8569718
    ##   0.3  3          0.8               0.50       100      0.8577412
    ##   0.3  3          0.8               0.50       150      0.8555909
    ##   0.3  3          0.8               0.75        50      0.8666968
    ##   0.3  3          0.8               0.75       100      0.8679478
    ##   0.3  3          0.8               0.75       150      0.8657692
    ##   0.3  3          0.8               1.00        50      0.8692450
    ##   0.3  3          0.8               1.00       100      0.8682213
    ##   0.3  3          0.8               1.00       150      0.8666696
    ##   0.4  1          0.6               0.50        50      0.8425311
    ##   0.4  1          0.6               0.50       100      0.8448631
    ##   0.4  1          0.6               0.50       150      0.8416019
    ##   0.4  1          0.6               0.75        50      0.8463446
    ##   0.4  1          0.6               0.75       100      0.8473522
    ##   0.4  1          0.6               0.75       150      0.8460595
    ##   0.4  1          0.6               1.00        50      0.8479967
    ##   0.4  1          0.6               1.00       100      0.8532299
    ##   0.4  1          0.6               1.00       150      0.8525566
    ##   0.4  1          0.8               0.50        50      0.8440438
    ##   0.4  1          0.8               0.50       100      0.8419832
    ##   0.4  1          0.8               0.50       150      0.8404161
    ##   0.4  1          0.8               0.75        50      0.8446820
    ##   0.4  1          0.8               0.75       100      0.8459402
    ##   0.4  1          0.8               0.75       150      0.8461195
    ##   0.4  1          0.8               1.00        50      0.8485550
    ##   0.4  1          0.8               1.00       100      0.8531433
    ##   0.4  1          0.8               1.00       150      0.8518723
    ##   0.4  2          0.6               0.50        50      0.8544268
    ##   0.4  2          0.6               0.50       100      0.8530507
    ##   0.4  2          0.6               0.50       150      0.8497518
    ##   0.4  2          0.6               0.75        50      0.8575961
    ##   0.4  2          0.6               0.75       100      0.8579953
    ##   0.4  2          0.6               0.75       150      0.8584598
    ##   0.4  2          0.6               1.00        50      0.8656819
    ##   0.4  2          0.6               1.00       100      0.8663000
    ##   0.4  2          0.6               1.00       150      0.8648085
    ##   0.4  2          0.8               0.50        50      0.8575572
    ##   0.4  2          0.8               0.50       100      0.8526151
    ##   0.4  2          0.8               0.50       150      0.8502475
    ##   0.4  2          0.8               0.75        50      0.8541960
    ##   0.4  2          0.8               0.75       100      0.8554337
    ##   0.4  2          0.8               0.75       150      0.8595932
    ##   0.4  2          0.8               1.00        50      0.8633098
    ##   0.4  2          0.8               1.00       100      0.8662876
    ##   0.4  2          0.8               1.00       150      0.8646191
    ##   0.4  3          0.6               0.50        50      0.8535450
    ##   0.4  3          0.6               0.50       100      0.8557811
    ##   0.4  3          0.6               0.50       150      0.8530422
    ##   0.4  3          0.6               0.75        50      0.8605820
    ##   0.4  3          0.6               0.75       100      0.8591717
    ##   0.4  3          0.6               0.75       150      0.8590011
    ##   0.4  3          0.6               1.00        50      0.8659112
    ##   0.4  3          0.6               1.00       100      0.8637531
    ##   0.4  3          0.6               1.00       150      0.8620481
    ##   0.4  3          0.8               0.50        50      0.8534096
    ##   0.4  3          0.8               0.50       100      0.8542570
    ##   0.4  3          0.8               0.50       150      0.8550837
    ##   0.4  3          0.8               0.75        50      0.8611486
    ##   0.4  3          0.8               0.75       100      0.8599474
    ##   0.4  3          0.8               0.75       150      0.8603924
    ##   0.4  3          0.8               1.00        50      0.8654627
    ##   0.4  3          0.8               1.00       100      0.8646948
    ##   0.4  3          0.8               1.00       150      0.8633321
    ##   Sens       Spec     
    ##   0.8613636  0.6839327
    ##   0.8613636  0.6852929
    ##   0.8695455  0.6963232
    ##   0.8695455  0.6802559
    ##   0.8704545  0.6831919
    ##   0.8695455  0.6978182
    ##   0.8668182  0.6852929
    ##   0.8754545  0.6897239
    ##   0.8818182  0.6970505
    ##   0.8609091  0.6707609
    ##   0.8627273  0.6751650
    ##   0.8627273  0.6831515
    ##   0.8663636  0.6758519
    ##   0.8722727  0.6838519
    ##   0.8686364  0.6933603
    ##   0.8672727  0.6860471
    ##   0.8759091  0.6941010
    ##   0.8768182  0.6955825
    ##   0.8822727  0.6970236
    ##   0.8754545  0.7086869
    ##   0.8631818  0.7101145
    ##   0.8840909  0.6846734
    ##   0.8831818  0.7101414
    ##   0.8777273  0.7247003
    ##   0.8981818  0.6853064
    ##   0.8950000  0.7181010
    ##   0.8881818  0.7226263
    ##   0.8813636  0.6874882
    ##   0.8713636  0.7115690
    ##   0.8672727  0.7196364
    ##   0.8850000  0.6977912
    ##   0.8836364  0.7101414
    ##   0.8840909  0.7204175
    ##   0.8868182  0.6853333
    ##   0.8868182  0.7189630
    ##   0.8827273  0.7262492
    ##   0.8713636  0.7057912
    ##   0.8650000  0.7167138
    ##   0.8600000  0.7175488
    ##   0.8822727  0.7057778
    ##   0.8736364  0.7305724
    ##   0.8659091  0.7248081
    ##   0.8909091  0.7174411
    ##   0.8840909  0.7233266
    ##   0.8795455  0.7306667
    ##   0.8695455  0.7050505
    ##   0.8654545  0.7153131
    ##   0.8618182  0.7153939
    ##   0.8827273  0.7188552
    ##   0.8686364  0.7335758
    ##   0.8668182  0.7306667
    ##   0.8877273  0.7123232
    ##   0.8818182  0.7218721
    ##   0.8750000  0.7240943
    ##   0.8586364  0.6758114
    ##   0.8559091  0.6978182
    ##   0.8609091  0.7014411
    ##   0.8640909  0.6846195
    ##   0.8722727  0.6919057
    ##   0.8759091  0.7057912
    ##   0.8700000  0.6824108
    ##   0.8772727  0.6992189
    ##   0.8790909  0.7006869
    ##   0.8677273  0.6736700
    ##   0.8659091  0.6896700
    ##   0.8668182  0.6941145
    ##   0.8663636  0.6802155
    ##   0.8713636  0.6831919
    ##   0.8659091  0.6985320
    ##   0.8681818  0.6838653
    ##   0.8795455  0.6919461
    ##   0.8759091  0.7028552
    ##   0.8686364  0.7044040
    ##   0.8650000  0.7072862
    ##   0.8613636  0.7094680
    ##   0.8795455  0.6992593
    ##   0.8718182  0.7212256
    ##   0.8659091  0.7190842
    ##   0.8840909  0.7072593
    ##   0.8822727  0.7204040
    ##   0.8804545  0.7269899
    ##   0.8736364  0.7057912
    ##   0.8659091  0.7101953
    ##   0.8609091  0.7109630
    ##   0.8840909  0.6985724
    ##   0.8777273  0.7145859
    ##   0.8713636  0.7204310
    ##   0.8918182  0.6984377
    ##   0.8895455  0.7189764
    ##   0.8877273  0.7306532
    ##   0.8600000  0.7080404
    ##   0.8500000  0.7095488
    ##   0.8468182  0.7131852
    ##   0.8668182  0.7131178
    ##   0.8677273  0.7262896
    ##   0.8600000  0.7204444
    ##   0.8881818  0.7160000
    ##   0.8763636  0.7233939
    ##   0.8677273  0.7263300
    ##   0.8640909  0.7072189
    ##   0.8609091  0.7167273
    ##   0.8563636  0.7188956
    ##   0.8781818  0.7233401
    ##   0.8668182  0.7204040
    ##   0.8663636  0.7182492
    ##   0.8840909  0.7219125
    ##   0.8745455  0.7313939
    ##   0.8727273  0.7329024
    ## 
    ## Tuning parameter 'gamma' was held constant at a value of 0
    ## 
    ## Tuning parameter 'min_child_weight' was held constant at a value of 1
    ## ROC was used to select the optimal model using the largest value.
    ## The final values used for the model were nrounds = 50, max_depth = 3,
    ##  eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1
    ##  and subsample = 1.

``` r
xgb_pred_prob <- predict(xgb_model, testing, type = 'prob')
xgb.ROC <- roc(response = testing$Survived,
               predictor = xgb_pred_prob$X1,
               levels = levels(testing$Survived))
plot(xgb.ROC, type="S", col="red"); text(x=0, y=.25, labels=paste("AUC =", round(xgb.ROC$auc, 4)))
```

![](Day030_files/figure-markdown_github/unnamed-chunk-11-1.png)

GBDT+LR

``` r
train_encode <- xgb.create.features(xgb_model$finalModel, as.matrix(train[,1:(ncol(train)-1)]))
train_encode <- as.data.frame(as.matrix(train_encode)) 
# 新特徵名稱可能存在重複，需重新命名
names(train_encode) <- paste("f",1:ncol(train_encode),sep='')

train_encode$Survived <- train$Survived
training_encode <- train_encode[inTrain,]; testing_encode <- train_encode[-inTrain,]

control <- trainControl(method="cv", number=5,classProbs = TRUE, summaryFunction=twoClassSummary)
lr_model <- train(Survived~., data=training_encode, method="glmnet", metric="ROC", trControl=control)
lr_model
```

    ## glmnet 
    ## 
    ## 714 samples
    ## 292 predictors
    ##   2 classes: 'X0', 'X1' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 571, 571, 571, 572, 571 
    ## Resampling results across tuning parameters:
    ## 
    ##   alpha  lambda        ROC        Sens       Spec     
    ##   0.10   0.0005328059  0.9551661  0.9000000  0.8720539
    ##   0.10   0.0053280595  0.9683968  0.9318182  0.8793266
    ##   0.10   0.0532805950  0.9641682  0.9454545  0.8465993
    ##   0.55   0.0005328059  0.9537963  0.8977273  0.8719192
    ##   0.55   0.0053280595  0.9635262  0.9363636  0.8647811
    ##   0.55   0.0532805950  0.9255491  0.9522727  0.7334680
    ##   1.00   0.0005328059  0.9489470  0.9022727  0.8465993
    ##   1.00   0.0053280595  0.9562489  0.9295455  0.8429630
    ##   1.00   0.0532805950  0.8902326  0.9681818  0.6349495
    ## 
    ## ROC was used to select the optimal model using the largest value.
    ## The final values used for the model were alpha = 0.1 and lambda
    ##  = 0.005328059.

``` r
lr_pred_prob <- predict(lr_model, testing_encode, type = 'prob')
lr.ROC <- roc(response = testing_encode$Survived,
               predictor = lr_pred_prob$X1,
               levels = levels(testing_encode$Survived))
plot(lr.ROC, type="S", col="red"); text(x=0, y=.25, labels=paste("AUC =", round(lr.ROC$auc, 4)))
```

![](Day030_files/figure-markdown_github/unnamed-chunk-12-1.png)

作業1
-----

請對照範例，完成隨機森林的鐵達尼生存率預測，以及對應的葉編碼+邏輯斯迴歸

RF

``` r
control <- trainControl(method="repeatedcv", number=5, repeats=5, classProbs = TRUE, summaryFunction=twoClassSummary)
rf_model <- train(Survived ~ ., data = training, method = 'rf', trControl = control, verbose = F, metric = 'ROC')
rf_model
```

    ## Random Forest 
    ## 
    ## 714 samples
    ##  10 predictor
    ##   2 classes: 'X0', 'X1' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 5 times) 
    ## Summary of sample sizes: 571, 571, 571, 571, 572, 571, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  ROC        Sens       Spec     
    ##    2    0.8766694  0.8995455  0.6909899
    ##    6    0.8732174  0.8968182  0.7070707
    ##   10    0.8690504  0.8913636  0.7070168
    ## 
    ## ROC was used to select the optimal model using the largest value.
    ## The final value used for the model was mtry = 2.

``` r
rf_pred_prob <- predict(rf_model, testing, type = 'prob')
rf.ROC <- roc(response = testing$Survived,
               predictor = rf_pred_prob$X1,
               levels = levels(testing$Survived))
plot(rf.ROC, type="S", col="red"); text(x=0, y=.25, labels=paste("AUC =", round(rf.ROC$auc, 4)))
```

![](Day030_files/figure-markdown_github/unnamed-chunk-13-1.png)

RF+LR

``` r
train_encode2 <- xgb.create.features(rf_model$finalModel, as.matrix(train[,1:(ncol(train)-1)]))
train_encode2 <- as.data.frame(as.matrix(train_encode2)) 
# 新特徵名稱可能存在重複，需重新命名
names(train_encode2) <- paste("f",1:ncol(train_encode2),sep='')

train_encode2$Survived <- train$Survived
training_encode2 <- train_encode2[inTrain,]; testing_encode2 <- train_encode2[-inTrain,]

control <- trainControl(method="cv", number=5,classProbs = TRUE, summaryFunction=twoClassSummary)
lr_model <- train(Survived~., data=training_encode2, method="glmnet", metric="ROC", trControl=control)
lr_model
```

    ## glmnet 
    ## 
    ## 714 samples
    ##  12 predictor
    ##   2 classes: 'X0', 'X1' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 572, 571, 571, 571, 571 
    ## Resampling results across tuning parameters:
    ## 
    ##   alpha  lambda        ROC        Sens       Spec     
    ##   0.10   0.0008901432  0.9607155  0.9954545  0.9014815
    ##   0.10   0.0089014318  0.9551018  0.9954545  0.9014815
    ##   0.10   0.0890143182  0.9319659  0.9954545  0.9014815
    ##   0.55   0.0008901432  0.9605494  0.9954545  0.9014815
    ##   0.55   0.0089014318  0.9586027  0.9954545  0.9014815
    ##   0.55   0.0890143182  0.9484680  0.9954545  0.9014815
    ##   1.00   0.0008901432  0.9602571  0.9954545  0.9014815
    ##   1.00   0.0089014318  0.9579553  0.9954545  0.9014815
    ##   1.00   0.0890143182  0.9484680  0.9954545  0.9014815
    ## 
    ## ROC was used to select the optimal model using the largest value.
    ## The final values used for the model were alpha = 0.1 and lambda
    ##  = 0.0008901432.

``` r
lr_pred_prob <- predict(lr_model, testing_encode, type = 'prob')
lr.ROC <- roc(response = testing_encode2$Survived,
               predictor = lr_pred_prob$X1,
               levels = levels(testing_encode2$Survived))
plot(lr.ROC, type="S", col="red"); text(x=0, y=.25, labels=paste("AUC =", round(lr.ROC$auc, 4)))
```

![](Day030_files/figure-markdown_github/unnamed-chunk-14-1.png)
