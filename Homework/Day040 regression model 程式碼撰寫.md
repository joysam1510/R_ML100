Day040
================

caret::glmnet: <https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net>

``` r
library(mlbench)
library(tidyverse)
library(caret)
```

練習時間
--------

請使用其他資料集 (boston, wine)，並調整不同的 alpha 來觀察模型訓練的情形。

BostonHousing dataset

data loading

``` r
data(BostonHousing)
str(BostonHousing)
```

    ## 'data.frame':    506 obs. of  14 variables:
    ##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
    ##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
    ##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
    ##  $ chas   : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
    ##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
    ##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
    ##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
    ##  $ rad    : num  1 2 2 3 3 3 5 5 5 5 ...
    ##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
    ##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
    ##  $ b      : num  397 397 393 395 397 ...
    ##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
    ##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...

data partition

``` r
intrain <- sample(nrow(BostonHousing), nrow(BostonHousing)*.8)
train <- BostonHousing[intrain,]; test <- BostonHousing[-intrain,]
```

linear regression

``` r
control <- trainControl(method="cv", number=5)
fit_model <- train(medv~., data=train, method="lm", metric="RMSE", trControl=control)
fit_model
```

    ## Linear Regression 
    ## 
    ## 404 samples
    ##  13 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 324, 323, 322, 324, 323 
    ## Resampling results:
    ## 
    ##   RMSE      Rsquared   MAE    
    ##   4.649084  0.7391866  3.36811
    ## 
    ## Tuning parameter 'intercept' was held constant at a value of TRUE

``` r
fit_pred <- predict(fit_model, test)
MSE <- mean((fit_pred-test$medv)^2)
MSE
```

    ## [1] 27.49695

lasso regression

``` r
control <- trainControl(method="cv", number=5)
lasso_model <- train(medv~., data=train, method="glmnet", trControl=control, tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
lasso_model
```

    ## glmnet 
    ## 
    ## 404 samples
    ##  13 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 322, 323, 324, 324, 323 
    ## Resampling results across tuning parameters:
    ## 
    ##   lambda  RMSE      Rsquared   MAE     
    ##   0.001   4.826354  0.7365098  3.459741
    ##   0.002   4.826354  0.7365098  3.459741
    ##   0.003   4.826354  0.7365098  3.459741
    ##   0.004   4.826354  0.7365098  3.459741
    ##   0.005   4.826354  0.7365098  3.459741
    ##   0.006   4.826342  0.7365105  3.459629
    ##   0.007   4.826154  0.7365296  3.458733
    ##   0.008   4.825829  0.7365670  3.457470
    ##   0.009   4.825489  0.7366073  3.456076
    ##   0.010   4.825250  0.7366377  3.454744
    ##   0.011   4.824981  0.7366714  3.453553
    ##   0.012   4.824733  0.7367030  3.452478
    ##   0.013   4.824515  0.7367330  3.451400
    ##   0.014   4.824308  0.7367620  3.450386
    ##   0.015   4.824113  0.7367893  3.449385
    ##   0.016   4.823911  0.7368165  3.448368
    ##   0.017   4.823728  0.7368427  3.447317
    ##   0.018   4.823557  0.7368683  3.446282
    ##   0.019   4.823397  0.7368924  3.445255
    ##   0.020   4.823201  0.7369203  3.444199
    ##   0.021   4.823010  0.7369474  3.443169
    ##   0.022   4.822986  0.7369572  3.442316
    ##   0.023   4.822988  0.7369647  3.441476
    ##   0.024   4.823016  0.7369696  3.440639
    ##   0.025   4.823040  0.7369747  3.439804
    ##   0.026   4.823066  0.7369796  3.438982
    ##   0.027   4.823119  0.7369823  3.438240
    ##   0.028   4.823197  0.7369824  3.437531
    ##   0.029   4.823298  0.7369798  3.436850
    ##   0.030   4.823385  0.7369776  3.436150
    ##   0.031   4.823486  0.7369741  3.435464
    ##   0.032   4.823633  0.7369677  3.434856
    ##   0.033   4.823835  0.7369580  3.434303
    ##   0.034   4.824048  0.7369474  3.433743
    ##   0.035   4.824275  0.7369355  3.433229
    ##   0.036   4.824517  0.7369222  3.432789
    ##   0.037   4.824769  0.7369079  3.432381
    ##   0.038   4.825032  0.7368924  3.432017
    ##   0.039   4.825293  0.7368762  3.431647
    ##   0.040   4.825565  0.7368590  3.431277
    ##   0.041   4.825848  0.7368405  3.430899
    ##   0.042   4.826147  0.7368206  3.430525
    ##   0.043   4.826467  0.7367987  3.430162
    ##   0.044   4.826797  0.7367757  3.429798
    ##   0.045   4.827138  0.7367517  3.429444
    ##   0.046   4.827488  0.7367268  3.429087
    ##   0.047   4.827847  0.7367012  3.428720
    ##   0.048   4.828216  0.7366745  3.428354
    ##   0.049   4.828597  0.7366466  3.427982
    ##   0.050   4.828989  0.7366175  3.427607
    ##   0.051   4.829400  0.7365859  3.427244
    ##   0.052   4.829822  0.7365530  3.426883
    ##   0.053   4.830256  0.7365190  3.426524
    ##   0.054   4.830698  0.7364840  3.426171
    ##   0.055   4.831152  0.7364480  3.425827
    ##   0.056   4.831609  0.7364131  3.425501
    ##   0.057   4.832077  0.7363771  3.425186
    ##   0.058   4.832556  0.7363399  3.424927
    ##   0.059   4.833047  0.7363014  3.424688
    ##   0.060   4.833550  0.7362617  3.424473
    ##   0.061   4.834073  0.7362179  3.424296
    ##   0.062   4.834610  0.7361712  3.424123
    ##   0.063   4.835158  0.7361234  3.423951
    ##   0.064   4.835716  0.7360745  3.423782
    ##   0.065   4.836283  0.7360246  3.423647
    ##   0.066   4.836861  0.7359737  3.423513
    ##   0.067   4.837497  0.7359155  3.423434
    ##   0.068   4.838161  0.7358539  3.423380
    ##   0.069   4.838836  0.7357912  3.423357
    ##   0.070   4.839521  0.7357274  3.423339
    ##   0.071   4.840217  0.7356623  3.423319
    ##   0.072   4.840923  0.7355960  3.423298
    ##   0.073   4.841637  0.7355288  3.423275
    ##   0.074   4.842349  0.7354615  3.423257
    ##   0.075   4.843071  0.7353931  3.423276
    ##   0.076   4.843803  0.7353235  3.423295
    ##   0.077   4.844546  0.7352529  3.423350
    ##   0.078   4.845298  0.7351812  3.423412
    ##   0.079   4.846059  0.7351085  3.423474
    ##   0.080   4.846832  0.7350345  3.423537
    ##   0.081   4.847619  0.7349591  3.423602
    ##   0.082   4.848416  0.7348825  3.423667
    ##   0.083   4.849223  0.7348048  3.423733
    ##   0.084   4.850040  0.7347260  3.423798
    ##   0.085   4.850859  0.7346470  3.423862
    ##   0.086   4.851688  0.7345670  3.423926
    ##   0.087   4.852527  0.7344858  3.424012
    ##   0.088   4.853376  0.7344032  3.424104
    ##   0.089   4.854235  0.7343187  3.424193
    ##   0.090   4.855103  0.7342331  3.424281
    ##   0.091   4.855982  0.7341463  3.424415
    ##   0.092   4.856870  0.7340583  3.424564
    ##   0.093   4.857788  0.7339662  3.424712
    ##   0.094   4.858727  0.7338715  3.424860
    ##   0.095   4.859675  0.7337756  3.425008
    ##   0.096   4.860633  0.7336786  3.425157
    ##   0.097   4.861662  0.7335784  3.425406
    ##   0.098   4.862733  0.7334758  3.425714
    ##   0.099   4.863814  0.7333721  3.426022
    ##   0.100   4.864905  0.7332672  3.426331
    ## 
    ## Tuning parameter 'alpha' was held constant at a value of 1
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were alpha = 1 and lambda = 0.022.

``` r
# best parameter
lasso_model$bestTune
```

    ##    alpha lambda
    ## 22     1  0.022

``` r
# best coefficient
coef(lasso_model$finalModel, lasso_model$bestTune$lambda)
```

    ## 14 x 1 sparse Matrix of class "dgCMatrix"
    ##                        1
    ## (Intercept)  34.14753196
    ## crim         -0.07367857
    ## zn            0.04635058
    ## indus         .         
    ## chas1         2.08920057
    ## nox         -15.23196359
    ## rm            3.90333124
    ## age           .         
    ## dis          -1.50033821
    ## rad           0.28194987
    ## tax          -0.01156906
    ## ptratio      -0.93220278
    ## b             0.01096627
    ## lstat        -0.55939531

``` r
lasso_pred <- predict(lasso_model, test)
MSE <- mean((lasso_pred-test$medv)^2)
MSE
```

    ## [1] 27.4079

ridge regression

``` r
control <- trainControl(method="cv", number=5)
ridge_model <- train(medv~., data=train, method="glmnet", trControl=control, tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
ridge_model
```

    ## glmnet 
    ## 
    ## 404 samples
    ##  13 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 323, 324, 323, 323, 323 
    ## Resampling results across tuning parameters:
    ## 
    ##   lambda  RMSE      Rsquared   MAE     
    ##   0.001   4.830421  0.7387571  3.297849
    ##   0.002   4.830421  0.7387571  3.297849
    ##   0.003   4.830421  0.7387571  3.297849
    ##   0.004   4.830421  0.7387571  3.297849
    ##   0.005   4.830421  0.7387571  3.297849
    ##   0.006   4.830421  0.7387571  3.297849
    ##   0.007   4.830421  0.7387571  3.297849
    ##   0.008   4.830421  0.7387571  3.297849
    ##   0.009   4.830421  0.7387571  3.297849
    ##   0.010   4.830421  0.7387571  3.297849
    ##   0.011   4.830421  0.7387571  3.297849
    ##   0.012   4.830421  0.7387571  3.297849
    ##   0.013   4.830421  0.7387571  3.297849
    ##   0.014   4.830421  0.7387571  3.297849
    ##   0.015   4.830421  0.7387571  3.297849
    ##   0.016   4.830421  0.7387571  3.297849
    ##   0.017   4.830421  0.7387571  3.297849
    ##   0.018   4.830421  0.7387571  3.297849
    ##   0.019   4.830421  0.7387571  3.297849
    ##   0.020   4.830421  0.7387571  3.297849
    ##   0.021   4.830421  0.7387571  3.297849
    ##   0.022   4.830421  0.7387571  3.297849
    ##   0.023   4.830421  0.7387571  3.297849
    ##   0.024   4.830421  0.7387571  3.297849
    ##   0.025   4.830421  0.7387571  3.297849
    ##   0.026   4.830421  0.7387571  3.297849
    ##   0.027   4.830421  0.7387571  3.297849
    ##   0.028   4.830421  0.7387571  3.297849
    ##   0.029   4.830421  0.7387571  3.297849
    ##   0.030   4.830421  0.7387571  3.297849
    ##   0.031   4.830421  0.7387571  3.297849
    ##   0.032   4.830421  0.7387571  3.297849
    ##   0.033   4.830421  0.7387571  3.297849
    ##   0.034   4.830421  0.7387571  3.297849
    ##   0.035   4.830421  0.7387571  3.297849
    ##   0.036   4.830421  0.7387571  3.297849
    ##   0.037   4.830421  0.7387571  3.297849
    ##   0.038   4.830421  0.7387571  3.297849
    ##   0.039   4.830421  0.7387571  3.297849
    ##   0.040   4.830421  0.7387571  3.297849
    ##   0.041   4.830421  0.7387571  3.297849
    ##   0.042   4.830421  0.7387571  3.297849
    ##   0.043   4.830421  0.7387571  3.297849
    ##   0.044   4.830421  0.7387571  3.297849
    ##   0.045   4.830421  0.7387571  3.297849
    ##   0.046   4.830421  0.7387571  3.297849
    ##   0.047   4.830421  0.7387571  3.297849
    ##   0.048   4.830421  0.7387571  3.297849
    ##   0.049   4.830421  0.7387571  3.297849
    ##   0.050   4.830421  0.7387571  3.297849
    ##   0.051   4.830421  0.7387571  3.297849
    ##   0.052   4.830421  0.7387571  3.297849
    ##   0.053   4.830421  0.7387571  3.297849
    ##   0.054   4.830421  0.7387571  3.297849
    ##   0.055   4.830421  0.7387571  3.297849
    ##   0.056   4.830421  0.7387571  3.297849
    ##   0.057   4.830421  0.7387571  3.297849
    ##   0.058   4.830421  0.7387571  3.297849
    ##   0.059   4.830421  0.7387571  3.297849
    ##   0.060   4.830421  0.7387571  3.297849
    ##   0.061   4.830421  0.7387571  3.297849
    ##   0.062   4.830421  0.7387571  3.297849
    ##   0.063   4.830421  0.7387571  3.297849
    ##   0.064   4.830421  0.7387571  3.297849
    ##   0.065   4.830421  0.7387571  3.297849
    ##   0.066   4.830421  0.7387571  3.297849
    ##   0.067   4.830421  0.7387571  3.297849
    ##   0.068   4.830421  0.7387571  3.297849
    ##   0.069   4.830421  0.7387571  3.297849
    ##   0.070   4.830421  0.7387571  3.297849
    ##   0.071   4.830421  0.7387571  3.297849
    ##   0.072   4.830421  0.7387571  3.297849
    ##   0.073   4.830421  0.7387571  3.297849
    ##   0.074   4.830421  0.7387571  3.297849
    ##   0.075   4.830421  0.7387571  3.297849
    ##   0.076   4.830421  0.7387571  3.297849
    ##   0.077   4.830421  0.7387571  3.297849
    ##   0.078   4.830421  0.7387571  3.297849
    ##   0.079   4.830421  0.7387571  3.297849
    ##   0.080   4.830421  0.7387571  3.297849
    ##   0.081   4.830421  0.7387571  3.297849
    ##   0.082   4.830421  0.7387571  3.297849
    ##   0.083   4.830421  0.7387571  3.297849
    ##   0.084   4.830421  0.7387571  3.297849
    ##   0.085   4.830421  0.7387571  3.297849
    ##   0.086   4.830421  0.7387571  3.297849
    ##   0.087   4.830421  0.7387571  3.297849
    ##   0.088   4.830421  0.7387571  3.297849
    ##   0.089   4.830421  0.7387571  3.297849
    ##   0.090   4.830421  0.7387571  3.297849
    ##   0.091   4.830421  0.7387571  3.297849
    ##   0.092   4.830421  0.7387571  3.297849
    ##   0.093   4.830421  0.7387571  3.297849
    ##   0.094   4.830421  0.7387571  3.297849
    ##   0.095   4.830421  0.7387571  3.297849
    ##   0.096   4.830421  0.7387571  3.297849
    ##   0.097   4.830421  0.7387571  3.297849
    ##   0.098   4.830421  0.7387571  3.297849
    ##   0.099   4.830421  0.7387571  3.297849
    ##   0.100   4.830421  0.7387571  3.297849
    ## 
    ## Tuning parameter 'alpha' was held constant at a value of 0
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were alpha = 0 and lambda = 0.1.

``` r
# best parameter
ridge_model$bestTune
```

    ##     alpha lambda
    ## 100     0    0.1

``` r
# best coefficient
coef(ridge_model$finalModel, ridge_model$bestTune$lambda)
```

    ## 14 x 1 sparse Matrix of class "dgCMatrix"
    ##                         1
    ## (Intercept)  26.793797278
    ## crim         -0.064124894
    ## zn            0.034320962
    ## indus        -0.039007306
    ## chas1         2.297252482
    ## nox         -10.971487287
    ## rm            4.097463209
    ## age          -0.002161584
    ## dis          -1.156284614
    ## rad           0.164642609
    ## tax          -0.006451686
    ## ptratio      -0.850433721
    ## b             0.010813536
    ## lstat        -0.502717527

``` r
ridge_pred <- predict(ridge_model, test)
MSE <- mean((ridge_pred-test$medv)^2)
MSE
```

    ## [1] 26.83331
